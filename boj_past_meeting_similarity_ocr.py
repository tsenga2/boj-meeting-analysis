# -*- coding: utf-8 -*-
"""boj_past_meeting_similarity_OCR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VqHL0eVNHjSmN_Joo3K2itJwY9hGLGx-
"""

from google.colab import drive
drive.mount('/content/drive')

!apt-get update
!apt-get install -y tesseract-ocr tesseract-ocr-jpn poppler-utils
!pip install pdf2image pytesseract

# Second cell: Import libraries and set up base configuration
import requests
from bs4 import BeautifulSoup
import os
import re
import logging
from tqdm.notebook import tqdm
import time
from datetime import datetime

# Set up base directory in Google Drive
DRIVE_BASE_DIR = '/content/drive/MyDrive/BOJ_Records'
PDF_DIR = os.path.join(DRIVE_BASE_DIR, 'pdfs')

# Create directories
os.makedirs(PDF_DIR, exist_ok=True)

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(DRIVE_BASE_DIR, 'download_progress.log')),
        logging.StreamHandler()
    ]
)

# Third cell: Define helper functions
def download_pdf(url, filename, session, pbar=None):
    """
    Download a PDF file with progress tracking.
    """
    try:
        response = session.get(url, timeout=30)
        response.raise_for_status()

        with open(filename, 'wb') as f:
            f.write(response.content)

        if pbar:
            pbar.update(1)
            pbar.set_description(f"Downloaded {os.path.basename(filename)}")

        return True

    except requests.RequestException as e:
        logging.error(f"Failed to download: {url}")
        logging.error(f"Error: {str(e)}")
        return False

def get_record_pdfs(year, session):
    """
    Get PDFs from a specific year's record page.
    """
    base_url = "https://www.boj.or.jp"
    url = f"{base_url}/mopo/mpmsche_minu/record_{year}/index.htm"

    try:
        response = session.get(url, timeout=30)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')
        pdf_links = soup.find_all('a', href=re.compile(r'/mopo/mpmsche_minu/record_\d{4}/gjrk\d{6}a\.pdf$'))

        return [(f"{base_url}{link['href']}", os.path.basename(link['href'])) for link in pdf_links]

    except requests.RequestException as e:
        logging.error(f"Error fetching the webpage for year {year}: {str(e)}")
        return []

# Fourth cell: Define main processing function
def process_year(year, session=None):
    """
    Process a single year with progress tracking
    """
    if session is None:
        session = requests.Session()

    pdfs_downloaded = 0
    pdf_links = get_record_pdfs(year, session)

    if not pdf_links:
        print(f"No PDFs found for year {year}")
        return 0

    print(f"\nProcessing year {year} - Found {len(pdf_links)} PDFs")

    # Create progress bar for this year
    with tqdm(total=len(pdf_links), desc=f"Year {year}", leave=True) as pbar:
        for pdf_url, pdf_name in pdf_links:
            filename = os.path.join(PDF_DIR, f"{year}_{pdf_name}")

            if os.path.exists(filename):
                pbar.update(1)
                pbar.set_description(f"Skipped existing {pdf_name}")
                continue

            if download_pdf(pdf_url, filename, session, pbar):
                pdfs_downloaded += 1
                time.sleep(0.5)  # Prevent too rapid requests

    return pdfs_downloaded

# Fifth cell: Interactive year selection and processing
def main():
    print("BOJ PDF Downloader for Google Colab")
    print("===================================")

    # Get custom year range
    try:
        use_custom = input("Would you like to specify a custom year range? (y/n): ").lower().strip()
        if use_custom == 'y':
            start_year = int(input("Enter start year (1998-2014): "))
            end_year = int(input("Enter end year (1998-2014): "))
            if not (1998 <= start_year <= 2014 and 1998 <= end_year <= 2014 and start_year <= end_year):
                raise ValueError("Invalid year range")
        else:
            start_year = 1998
            end_year = 2014

        print(f"\nProcessing years {start_year} to {end_year}")

        # Initialize session
        session = requests.Session()
        total_downloaded = 0

        try:
            # Process each year
            for year in range(end_year, start_year - 1, -1):
                year_downloaded = process_year(year, session)
                total_downloaded += year_downloaded
                print(f"Year {year} complete - Downloaded {year_downloaded} new PDFs")

            print(f"\nDownload complete! Total new PDFs downloaded: {total_downloaded}")
            print(f"Files are saved in: {PDF_DIR}")

        finally:
            session.close()

    except ValueError as e:
        print(f"Error: {e}")
        print("Using default years: 1998-2014")
        main()

# Sixth cell: Run the main function
if __name__ == "__main__":
    main()

# Commented out IPython magic to ensure Python compatibility.
# # First Cell: Install required packages
# %%capture
# !apt-get update
# !apt-get install -y tesseract-ocr tesseract-ocr-jpn poppler-utils mecab libmecab-dev mecab-ipadic-utf8
# !pip install pdf2image pytesseract mecab-python3 pdfminer.six wordcloud matplotlib pandas scikit-learn numpy tqdm

# Second Cell: Modified analysis script
import os
import re
from collections import defaultdict, Counter
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
from datetime import datetime
import MeCab
from wordcloud import WordCloud
from pdf2image import convert_from_path
import pytesseract
from tqdm.notebook import tqdm

os.environ['MECABRC'] = "/etc/mecabrc"

def process_pdf_with_ocr(pdf_path, max_pages=5):
    """
    Convert PDF to images and perform OCR with Japanese language support.
    Args:
        pdf_path: Path to the PDF file
        max_pages: Maximum number of pages to process (default=5 for testing)
    """
    try:
        print(f"Converting {pdf_path} to images...")
        images = convert_from_path(pdf_path)
        total_pages = len(images)
        pages_to_process = min(max_pages, total_pages)
        print(f"Found {total_pages} pages total, processing first {pages_to_process} pages")

        full_text = []
        for i, image in enumerate(images[:pages_to_process], 1):
            print(f"Processing page {i}/{pages_to_process} (of {total_pages} total)")
            text = pytesseract.image_to_string(image, lang='jpn')
            full_text.append(text)

        return '\n'.join(full_text)

    except Exception as e:
        print(f"Error processing PDF {pdf_path}: {str(e)}")
        return ""

# The analysis function with fixed metric names
def analyze_boj_documents(pdf_dir, output_dir="analysis_output", max_pages=5):
    """Analyze BOJ documents including OCR processing"""
    os.makedirs(output_dir, exist_ok=True)
    data = []
    processed_count = 0

    # Get list of PDFs
    pdf_files = sorted([f for f in os.listdir(pdf_dir) if f.endswith('.pdf')])
    print(f"Found {len(pdf_files)} PDF files")
    print(f"Processing {max_pages} pages per document for testing")

    # Process each PDF with progress bar
    for filename in tqdm(pdf_files, desc="Processing PDFs"):
        pdf_path = os.path.join(pdf_dir, filename)
        print(f"\nProcessing file: {filename}")

        # Extract date from filename
        date_match = re.search(r'gjrk(\d{6})a', filename)
        if not date_match:
            print(f"No date match found in filename: {filename}")
            continue

        date_str = date_match.group(1)
        try:
            date = datetime.strptime(date_str, "%y%m%d").date()

            # Extract text using OCR (limited pages)
            text = process_pdf_with_ocr(pdf_path, max_pages=max_pages)

            if not text.strip():
                print(f"Warning: No text extracted from {filename}")
                continue

            # Process text with MeCab
            mecab = MeCab.Tagger("")
            processed_text = []
            node = mecab.parseToNode(text)
            while node:
                features = node.feature.split(',')
                if features[0] in ['名詞', '動詞', '形容詞']:
                    if features[0] == '動詞' and len(features) > 6:
                        processed_text.append(features[6])
                    else:
                        processed_text.append(node.surface)
                node = node.next

            processed_text = ' '.join(processed_text)

            # Extract metrics with consistent naming
            metrics = {
                'inflation_mentions': len(re.findall('インフレ|物価上昇', text)),
                'deflation_mentions': len(re.findall('デフレ|物価下落', text)),
                'fx_mentions': len(re.findall('為替|円相場|ドル円|ユーロ円', text)),
                'interest_rate_mentions': len(re.findall('金利|利率|利回り', text)),
                'economy_mentions': len(re.findall('景気|経済情勢|経済状況', text))
            }

            data.append({
                'date': date,
                'filename': filename,
                'text': processed_text,
                'pages_processed': min(max_pages, len(convert_from_path(pdf_path))),
                **metrics
            })

            processed_count += 1
            print(f"Successfully processed {filename} ({processed_count}/{len(pdf_files)})")

        except Exception as e:
            print(f"Error processing {filename}: {e}")
            continue

    # Check if we have any data
    if not data:
        print("No data was collected from PDFs")
        return pd.DataFrame()

    # Create DataFrame
    print(f"\nCreating DataFrame with {len(data)} entries")
    df = pd.DataFrame(data)
    df = df.sort_values('date')

    # Calculate text similarity
    if len(df) > 1:
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(df['text'])
        similarity_matrix = cosine_similarity(tfidf_matrix)
        df['similarity_with_previous'] = [0] + [similarity_matrix[i, i-1] for i in range(1, len(df))]

    # Create visualizations
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)

    # Similarity plot
    ax1.plot(df['date'], df['similarity_with_previous'], marker='o')
    ax1.set_title("Cosine Similarity Between Consecutive Monetary Policy Meetings")
    ax1.set_ylabel("Cosine Similarity")
    ax1.grid(True)

    # Key metrics plot - using consistent column names
    ax2.plot(df['date'], df['inflation_mentions'], label='Inflation', marker='o')
    ax2.plot(df['date'], df['deflation_mentions'], label='Deflation', marker='*')
    ax2.plot(df['date'], df['fx_mentions'], label='FX', marker='^')
    ax2.set_title("Mention Frequency of Key Economic Indicators")
    ax2.set_xlabel("Date")
    ax2.set_ylabel("Mention Frequency")
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "monetary_policy_analysis.png"))
    plt.show()
    plt.close()

    # Save results
    df.to_csv(os.path.join(output_dir, 'boj_analysis_results.csv'), index=False)

    return df

# Run the analysis
if __name__ == "__main__":
    pdf_dir = "/content/drive/MyDrive/BOJ_Records/pdfs"
    output_dir = "/content/drive/MyDrive/BOJ_Records/analysis"
    MAX_PAGES = 5

    print(f"Starting BOJ document analysis (testing with {MAX_PAGES} pages per document)...")
    results_df = analyze_boj_documents(pdf_dir, output_dir, max_pages=MAX_PAGES)

    if len(results_df) > 0:
        print("\nAnalysis complete!")
        print("\nSample of metrics:")
        display(results_df[['date', 'inflation_mentions', 'deflation_mentions',
                          'fx_mentions', 'similarity_with_previous']].head())

        print("\nSummary statistics:")
        display(results_df[['inflation_mentions', 'deflation_mentions',
                          'fx_mentions', 'interest_rate_mentions',
                          'economy_mentions']].describe())
    else:
        print("\nNo results were generated. Please check the error messages above.")

